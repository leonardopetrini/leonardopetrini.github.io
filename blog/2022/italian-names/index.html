<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Italian names generator ü§å (Part I) | Leonardo Petrini</title> <meta name="author" content="Leonardo Petrini"> <meta name="description" content="building an ML model for generating Italian names"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://leopetrini.me/blog/2022/italian-names/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KYRX4WM6CB"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-KYRX4WM6CB");</script> <meta name="twitter:site" content="@leopetrini_"> <meta name="twitter:creator" content="@leopetrini_"> <meta name="twitter:title" content="Italian names generator ü§å (Part I)"> <meta name="twitter:description" content="building an ML model for generating Italian names"> <meta name="twitter:card" content="summary"> <meta name="twitter:image" content="https://leopetrini.me/assets/img/pairwise_n_it_1.png"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Leonardo¬†</span>Petrini</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">my notes<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Italian names generator ü§å (Part I)</h1> <p class="post-meta">December 4, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> ¬† ¬∑ ¬† <a href="/blog/tag/nlp"> <i class="fas fa-hashtag fa-sm"></i> nlp</a> ¬† <a href="/blog/tag/machine-learning"> <i class="fas fa-hashtag fa-sm"></i> machine-learning</a> ¬† <a href="/blog/tag/n-grams"> <i class="fas fa-hashtag fa-sm"></i> n-grams</a> ¬† ¬† ¬∑ ¬† <a href="/blog/category/ml-projects"> <i class="fas fa-tag fa-sm"></i> ml-projects</a> ¬† <a href="/blog/category/blogposts"> <i class="fas fa-tag fa-sm"></i> blogposts</a> ¬† </p> </header> <article class="post-content"> <p>This series of posts is largely inspired by Andrej Karpathy‚Äôs <a href="https://github.com/karpathy/makemore" rel="external nofollow noopener" target="_blank"><em>makemore</em></a>. In my experience as an ML researcher, I‚Äôve never found a more clear and sharp teacher than Andrej, his ability to walk you through the basics without being pedantic, and give insightful comments on the way, I find it unique. I highly recommend his (ongoing) YouTube series on <a href="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" rel="external nofollow noopener" target="_blank">Neural Networks: from Zero to Hero</a>.</p> <p>So, let‚Äôs start! As the title suggests, we are going to build generative models for <em>Italian first names</em> that will learn from examples. Such models will be able to generate new words from scratch or complete a series of characters. I will make use of two names datasets publicly available on GitHub:</p> <ol> <li> <a href="https://gist.github.com/pdesterlich/2562329" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">names_1</code></a> contains ~9k first names of people living in Italy, not necessarily <em>strictly</em> Italian. There are e.g. some French names;</li> <li> <a href="https://github.com/filippotoso/nomi-cognomi-italiani/blob/master/json/nomi-italiani.json" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">names_2</code></a> contains ~1.7k first names.</li> </ol> <p>Some examples from <code class="language-plaintext highlighter-rouge">names_1</code>,</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ranuccio palmerio eustacchia quentalina gesuina azaea finaldo oriana 
</code></pre></div></div> <p>and <code class="language-plaintext highlighter-rouge">names_2</code>,</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>romoaldo donatella nicoletta aristeo natalia rainelda serafina susanna
</code></pre></div></div> <p>In the following, I will consider the larger dataset, <code class="language-plaintext highlighter-rouge">names_1</code>.</p> <p>The datasets, together with the notebook reproducing the results shown in this post can be found at <a href="https://github.com/leonardopetrini/learning-italian-names" rel="external nofollow noopener" target="_blank">github/leonardopetrini/learning-italian-names</a>.</p> <h2 id="0-th-order-model-random-guessing">0-th order model: random guessing</h2> <p>Let‚Äôs start with the simplest thing we could do, and a very bad baseline: <em>random guessing</em>. I call it a 0th-order method because we give the model no information about the statistics of characters in the words. This point will become clearer later on when we go to higher order.</p> <p>In this context, random guessing consists in taking a dictionary of all the characters that occur in our words dataset and sampling uniformly at random from them.</p> <p>The dictionary of chars appearing in <code class="language-plaintext highlighter-rouge">names_1</code> is the following</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>' ' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'
'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' '√†' '√®' '√¨' '√≤' '√π' 
</code></pre></div></div> <p>You can notice that to the 26 letters of the Latin alphabet we added accented vowels, and the blank, as some Italian names are composite, for a total of <code class="language-plaintext highlighter-rouge">32</code> chars.</p> <p>We add the char <code class="language-plaintext highlighter-rouge">'.'</code> to the dictionary as a placeholder indicating the word end, and append it to all words as well. This makes our dictionary of length <code class="language-plaintext highlighter-rouge">L=33</code>.</p> <p>For generating a word, we will sample any one character uniformly at random (i.e. with probability \(1/L \approx 0.03\)), until we sample the special char <code class="language-plaintext highlighter-rouge">'.'</code>, at which we stop. Samples from such a trivial model will look like this</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code> hxdwsemlxermj√πabp√≤ctq gmamqqvz .
bc√†enn√¨qklrjktrhaulwtkwjjjmcj.
agt.
√πzfcds√π√≤dso.
.
ybt√®√≤.
im√¨uopsgjzwmfekdq√†yh√®fl nlm√¨xsapcqyoo√≤wwy√≤sfvnncfedcwmvvwthfemgsvip√†oh√® vunxdd√†kfugmil√πixv√πccw√¨btrtnd√≤c√πk.
zsyszq√¨.
ktmxktnsypd fxpks pagzsra√†uy√¨yt√®jujj√†iqwjpaqw√®.
 ptazvuwgisdpmfkzmjvyrg√≤lsnaw√≤add√¨tupnkysnakyxh√≤jh√π.
</code></pre></div></div> <p>Clearly not Italian, nor words! ‚ùå</p> <p>Indeed, one of the first things we notice is that words are incredibly longer than typical names. This is because the <code class="language-plaintext highlighter-rouge">'.'</code> will occur, on average, every 33 characters, making the average word length =33, much larger than the average word length in <code class="language-plaintext highlighter-rouge">names_1</code> of <code class="language-plaintext highlighter-rouge">7.1</code>.</p> <p>We start to understand that the first necessary step is to account for the relative occurrence frequency of different characters.</p> <h2 id="1-st-order-model-average-char-occurrence">1-st order model: average char occurrence</h2> <p>In this section, we start to learn from data. As hinted above, the 1st order statistics we can get from our dataset consist of accounting for how often each element in the vocabulary appears in Italian names.</p> <p>To do so, we count how many times (<code class="language-plaintext highlighter-rouge">N</code>) each character appears. Then, we normalize the counting and obtain the probability <code class="language-plaintext highlighter-rouge">p</code> for char occurrence. These values are reported in the table below.</p> <table> <thead> <tr> <th>¬†</th> <th>space</th> <th>a</th> <th>b</th> <th>c</th> <th>d</th> <th>e</th> <th>f</th> <th>g</th> <th>h</th> <th>i</th> <th>j</th> <th>k</th> <th>l</th> <th>m</th> <th>n</th> <th>o</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">N</code></td> <td>102</td> <td>9116</td> <td>792</td> <td>1777</td> <td>2628</td> <td>5653</td> <td>1345</td> <td>1361</td> <td>209</td> <td>8380</td> <td>50</td> <td>38</td> <td>4720</td> <td>1979</td> <td>5487</td> <td>7585</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">p</code></td> <td>0.0014</td> <td>0.1237</td> <td>0.0107</td> <td>0.0241</td> <td>0.0357</td> <td>0.0767</td> <td>0.0183</td> <td>0.0185</td> <td>0.0028</td> <td>0.1137</td> <td>0.0007</td> <td>0.0005</td> <td>0.0641</td> <td>0.0269</td> <td>0.0745</td> <td>0.1029</td> </tr> </tbody> </table> <table> <thead> <tr> <th>¬†</th> <th>p</th> <th>q</th> <th>r</th> <th>s</th> <th>t</th> <th>u</th> <th>v</th> <th>w</th> <th>x</th> <th>y</th> <th>z</th> <th>√†</th> <th>√®</th> <th>√¨</th> <th>√≤</th> <th>√π</th> <th>.</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">N</code></td> <td>709</td> <td>73</td> <td>4884</td> <td>2147</td> <td>2558</td> <td>1141</td> <td>997</td> <td>65</td> <td>9</td> <td>26</td> <td>721</td> <td>4</td> <td>8</td> <td>3</td> <td>3</td> <td>4</td> <td>9111</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">p</code></td> <td>0.0096</td> <td>0.0010</td> <td>0.0663</td> <td>0.0291</td> <td>0.0347</td> <td>0.0155</td> <td>0.0135</td> <td>0.0009</td> <td>0.0001</td> <td>0.0004</td> <td>0.0098</td> <td>0.0001</td> <td>0.0001</td> <td>0.0000</td> <td>0.0000</td> <td>0.0001</td> <td>0.1236</td> </tr> </tbody> </table> <p><br><br> Notice that the special character <code class="language-plaintext highlighter-rouge">"."</code> appears 9111 times, which corresponds to the dataset size. Moreover, letters like <code class="language-plaintext highlighter-rouge">j, k, w, x, y</code> are very rare, while vowels are very common, except for the letter <code class="language-plaintext highlighter-rouge">u</code>.</p> <p>Similarly to what we did for random guessing, we sample new words by sequentially sampling characters, this time accordingly to <code class="language-plaintext highlighter-rouge">p</code>, until we hit <code class="language-plaintext highlighter-rouge">"."</code>. Here some results,</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>io.
ttlfo.
iarf.
iefniaaieanie.
uiaahl.
lvpcoroniaadi.
rv o.
inrealo.
aoieueaneaa.
b.
</code></pre></div></div> <p>Words length is now reasonable, and vowels appear much more often, as they should. Still, none of these words can be mistaken for an Italian name.</p> <h2 id="2-nd-order-model-pairwise-correlations">2-nd order model: pairwise correlations</h2> <p>Now we are starting to understand the game of incorporating data statistics into our model. In this step, we include 2nd order statistics, namely <em>pairwise correlations</em> between characters. This is to capture the fact that, e.g. the letter <code class="language-plaintext highlighter-rouge">n</code> often follows the letter <code class="language-plaintext highlighter-rouge">i</code>, <code class="language-plaintext highlighter-rouge">o</code> often follows <code class="language-plaintext highlighter-rouge">n</code> and so on‚Ä¶</p> <p>Similarly to the above case, this is done by making a table of all possible character pairs, computing their occurrence <code class="language-plaintext highlighter-rouge">N</code>, and normalizing it to obtain a probability <code class="language-plaintext highlighter-rouge">p</code>. Then, we look at the current character, we go to the corresponding row of <code class="language-plaintext highlighter-rouge">p</code> and sample the next char accordingly.</p> <p>Now you may ask: how do we choose the first character? To do that, we resort to a simple trick: we append to all words‚Äô beginnings our special <code class="language-plaintext highlighter-rouge">"."</code>, similarly to what we did for word ends. In this way, the row of <code class="language-plaintext highlighter-rouge">p</code> corresponding to <code class="language-plaintext highlighter-rouge">"."</code> will tell us what‚Äôs the probability that a given name starts with any of the letters in the vocabulary.</p> <p>The table <code class="language-plaintext highlighter-rouge">N</code> for char pairs is shown in the figure.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pairwise_n_it_1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pairwise_n_it_1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pairwise_n_it_1-1400.webp"></source> <img src="/assets/img/pairwise_n_it_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Pairs occurrence count $$N(c_i, c_{i+1})$$ </div> <p>Some features of Italian names start to emerge by looking at this table. In the 1st column, for example, we see that most names end with <code class="language-plaintext highlighter-rouge">a</code> or <code class="language-plaintext highlighter-rouge">o</code>, where the first is usually for feminine names, while the second is for masculine ones.</p> <p>Taking a few samples from our model we obtain</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vintona.
mada.
memefio.
o.
jalino.
ricaeto.
pelandinrazona.
wilio.
mia.
zia.
</code></pre></div></div> <p>Start to sound reasonable, don‚Äôt they?!</p> <h2 id="higher-order-models-n-grams">Higher order models: <em>n-grams</em> </h2> <p>In the literature, the models we are introducing here are called <a href="https://en.wikipedia.org/wiki/N-gram" rel="external nofollow noopener" target="_blank"><em>n-grams</em></a>, where <code class="language-plaintext highlighter-rouge">n</code> stands for the model <em>order</em>.</p> <p><strong><em>Note:</em></strong> How to sample from higher order models starts to be less intuitive, check the <em>post scriptum</em> below if you are interested in a more detailed walkthrough!</p> <p>Better and better results can be obtained by going to higher and higher order correlations, for example, for <code class="language-plaintext highlighter-rouge">n=3</code> we get,</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vilommanna.
cennelforena.
dino.
do.
cantino.
edo.
lipetto.
curgio.
medazia.
rano.
</code></pre></div></div> <p>‚ùì Could a non-native Italian speaker distinguish these from actual Italian names?</p> <p>It looks like by just replicating 3-rd order statistics from data, our model does a pretty good job! üëä Yet, <em>n-grams</em> have some limitations:</p> <ol> <li> <strong>Space.</strong> We need to store a matrix of size <code class="language-plaintext highlighter-rouge">L^n</code>, with <code class="language-plaintext highlighter-rouge">L</code> as the vocabulary size. On my laptop, I can get at most to <code class="language-plaintext highlighter-rouge">n=6</code> before getting to a memory error. For larger vocabulary sizes, this limit is reached even earlier (e.g. for English sentences, <code class="language-plaintext highlighter-rouge">L~10^4</code>).</li> </ol> <p>Here the <code class="language-plaintext highlighter-rouge">n=6</code> results:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>violanda.    1           
emerigo.     1           
cordelia.    1           
mirco.       1           
carolino.    1           
arina.       1           
azzurro.     1           
fiolomea.    0           
martano.     1           
flamiana.    0   
</code></pre></div></div> <ol start="2"> <li> <p><strong>Overfitting.</strong> In these results, I appended a <code class="language-plaintext highlighter-rouge">1</code> when the generated sample belongs to the original dataset. We see that the model learns by heart most of the examples and it can produce little variability beyond that. This is a manifestation of overfitting.</p> </li> <li> <p><strong>Long-range context.</strong> When the average word length gets large (<code class="language-plaintext highlighter-rouge">&gt;&gt;n</code>), n-grams fail to capture long-range correlations between characters.</p> </li> </ol> <p>Do modern artificial neural networks ü§ñ overcome these limitations and give better results? Moreover, how do we decide if a model is better than another, beyond anecdotal evidence?</p> <p>We will give answers to these questions in the following posts on this topic, stay tuned. üìª</p> <p><br><br></p> <h2 id="ps-how-to-generate-new-words-from-the-model"> <strong>PS:</strong> How to generate new words from the model</h2> <p>We briefly illustrate here how to generate new words given the model. We consider the case <code class="language-plaintext highlighter-rouge">n=3</code>. We built the matrix \(N(c_i, c_{i-1}, c_{i-2})\) by counting the occurrences of the three consecutive chars \(c_{i-2}c_{i-1}c_i\) in the data samples. Notice that all words in the dataset are now augmented with <code class="language-plaintext highlighter-rouge">n-1</code> leading <code class="language-plaintext highlighter-rouge">"."</code> and <code class="language-plaintext highlighter-rouge">1</code> trailing <code class="language-plaintext highlighter-rouge">"."</code>. This is to capture the statistics of the first <code class="language-plaintext highlighter-rouge">n-1</code> characters, which do not have <code class="language-plaintext highlighter-rouge">n-1</code> predecessors.</p> <p>We normalize the first dimension of \(N\) to obtain the conditional probability of a character given the previous <code class="language-plaintext highlighter-rouge">n-1</code>s,</p> \[P(c_i \,\vert\, c_{i-1}, c_{i-2}) = \frac{N(c_i, c_{i-1}, c_{i-2})}{\sum_{c_i} N(c_i, c_{i-1}, c_{i-2})}.\] <p>Finally, to sample a new word, we make use of our special character <code class="language-plaintext highlighter-rouge">"."</code>. The first char is sampled according to \(c_1 \sim P(c \,\vert\, .\,,\, .\,).\) Then, we sample \(c_2\) from \(P(c \,\vert\, c_1,\, .\,)\), \(c_3\) from \(P(c \,\vert\, c_2, c_1)\) and so on, until we sample the stopping char <code class="language-plaintext highlighter-rouge">"."</code>. We got our new word \(c_1c_2\dots c_m\), where \(c_m = \,.\,\).</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2023 Leonardo Petrini. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>