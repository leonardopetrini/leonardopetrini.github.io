<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Italian names generator ü§å (Part II) | Leonardo Petrini</title> <meta name="author" content="Leonardo Petrini"> <meta name="description" content="measuring the *goodness* of generative models: the likelihood"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://leopetrini.me/blog/2022/likelihood/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KYRX4WM6CB"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-KYRX4WM6CB");</script> <meta name="twitter:site" content="@leopetrini_"> <meta name="twitter:creator" content="@leopetrini_"> <meta name="twitter:title" content="Italian names generator ü§å (Part II)"> <meta name="twitter:description" content="measuring the *goodness* of generative models: the likelihood"> <meta name="twitter:card" content="summary"> <meta name="twitter:image" content="https://leopetrini.me/assets/img/likelihood-def.png"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Leonardo¬†</span>Petrini</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">my notes<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Italian names generator ü§å (Part II)</h1> <p class="post-meta">December 18, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> ¬† ¬∑ ¬† <a href="/blog/tag/nlp"> <i class="fas fa-hashtag fa-sm"></i> nlp</a> ¬† <a href="/blog/tag/machine-learning"> <i class="fas fa-hashtag fa-sm"></i> machine-learning</a> ¬† <a href="/blog/tag/likelihood"> <i class="fas fa-hashtag fa-sm"></i> likelihood</a> ¬† ¬† ¬∑ ¬† <a href="/blog/category/ml-projects"> <i class="fas fa-tag fa-sm"></i> ml-projects</a> ¬† <a href="/blog/category/blogposts"> <i class="fas fa-tag fa-sm"></i> blogposts</a> ¬† </p> </header> <article class="post-content"> <p>Back to our series about <strong>generating Italian names</strong>! <a href="/blog/2022/italian-names/">Last time</a> we built <em>n-gram</em> models for our data, seen that results improve as <code class="language-plaintext highlighter-rouge">n</code> gets larger, but also discussed their limitations. In this post, we are going to define a measure of model <em>goodness</em> to quantitatively assess which <code class="language-plaintext highlighter-rouge">n</code> works best with the data we have.</p> <p>The notebook reproducing the results shown in this post can be found here: <a href="https://github.com/leonardopetrini/learning-italian-names/blob/main/likelihood.ipynb" rel="external nofollow noopener" target="_blank">github/leonardopetrini/learning-italian-names/likelihood.ipynb</a>.</p> <h2 id="introducing-the-likelihood">Introducing the <em>likelihood</em> </h2> <p>A very natural measure of how good is our generative model in reproducing the statistical properties of the data is the probability that such data was generated from the model in the first place.</p> <p>Intuitively, if the data have structure, and our model was just randomly guessing, the probability that it generated the data would be very low, while a model correctly reproducing some data statistics (e.g. letter <code class="language-plaintext highlighter-rouge">n</code> often comes after letter <code class="language-plaintext highlighter-rouge">i</code>) would more <em>likely</em> have generated such data.</p> <p>The measure of <em>how likely</em> some data \(\widetilde X = \{\widetilde x^i\}_{i=1}^m\) are, given a model \(\mathcal{M}\), is commonly called <strong>likelihood</strong> and is defined as,</p> \[P(\widetilde X\,\vert\, \mathcal{M}) = \prod_{i=1}^m P(\ \widetilde x^i\,\vert\, \mathcal{M}),\] <p>where data points are assumed to be <em>independent</em> of each other (allowing to take the product) and <em>identically distributed</em> (allowing the same \(P\) for all samples). The larger the likelihood the better the model.</p> <p>For convenience, we usually work with the log of this quantity. Moreover, we divide by the total number of samples \(m\) and add a negative sign. This is because:</p> <ol> <li>The likelihood can take very tiny values as it is the product of \(m\) numbers in \([0, 1]\), and \(m\) can be arbitrarily large. Taking the \(\log\) makes it more tractable. Also, the \(\log\) is monotonic, hence it does not change the argmax of this function with respect to the model‚Äôs parameters.</li> <li>The \(\frac{1}{m}\) makes it an average quantity over samples so that it takes reasonable values (\(\mathcal{O}(1)\)) and it does not depend on the dataset size (in physics, we would call the average log-likelihood and <a href="https://en.wikipedia.org/wiki/Intensive_and_extensive_properties" rel="external nofollow noopener" target="_blank">intensive</a> quantity: a property of a system that does not depend on the system size).</li> <li>The negative sign makes it positive and bounded from below by zero. Also, it makes it interpretable as a <em>cost function</em>, a very common object in machine learning (ML).</li> </ol> <p>To summarize, we define the average <strong>negative log-likelihood</strong> as</p> <p>\begin{equation} \mathcal{L}(\mathcal{M}\,\vert\, \widetilde X) = -\frac{1}{m}\sum_{i=1}^m \log P(\ \widetilde x^i\,\vert\, \mathcal{M}) \label{eq:nll} \end{equation}</p> <p>where the order of \(\widetilde X\) and \(\mathcal{M}\) is reversed in the log-likelihood as it is commonly seen as a function of the model, given the data.</p> <p>This quantity is the cost function we aim at minimizing, and will support our choice of a model/set of parameters over another. In the following section, we‚Äôll see how to make proper use of it!</p> <h2 id="train-validate-test">Train, validate, test!</h2> <p><strong>‚ùì What makes a <em>good</em> machine learning model?</strong></p> <p>In the previous post, we have discussed two properties we would like our generative model to have:</p> <ul> <li>Ability to generate <strong>reasonable examples</strong> that look like the training ones;</li> <li>Ability to generate <strong>new examples</strong>, i.e. different than the one used for training.</li> </ul> <p>On the contrary, a model is bad if it generates examples that are reasonable but just copies of training samples. Or if it generates <strong>new</strong> examples that have nothing to do with the original ones (i.e. noise).</p> <p>In ML jargon, a <strong>good</strong> machine learning model has <strong>good</strong> generalization capabilities. This means that model <code class="language-plaintext highlighter-rouge">A</code> is better than model <code class="language-plaintext highlighter-rouge">B</code> if, after training, model <code class="language-plaintext highlighter-rouge">A</code> has a lower log-likelihood on <strong>new samples</strong> that did not belong to the training data, but that ideally come from the <strong>same probability distrubution</strong>.</p> <p>For this reason, we must evaluate the log-likelihood of our models on a different dataset than the one we have used for training.</p> <p>In particular, a common pipeline is to randomly split the dataset at our disposition into three parts (say 80%, 10%, 10%), the <strong>training</strong>, <strong>validation</strong> and <strong>test</strong> sets. The first is the one used for training the model, the second is used to have a sense of the model generalization capabilities, and accordingly tune the model <em>hyperparameters</em> (e.g. the value of <code class="language-plaintext highlighter-rouge">n</code> in <code class="language-plaintext highlighter-rouge">n</code>-grams). Finally, the third must be only sparingly used, usually at the very end of our ML pipeline, to get an unbiased estimate of the model performance.</p> <p>Splitting the dataset into three parts makes our ML pipeline more robust to <em>overfitting</em>, both with respect to the models‚Äô parameters and to the manually-tuned model hyperparameters. We‚Äôll see an example of overfitting below.</p> <h2 id="likelihood-and-word-models">Likelihood and word models</h2> <p>Now we have all the tools to start playing with our beloved Italian names!</p> <h3 id="a-toy-example">A toy example</h3> <p>As a warm-up, we introduce a <em>toy</em> words dataset with just three words:</p> \[\widetilde X = \{ab, bb, ba\}.\] <p>As we <a href="/blog/2022/italian-names/">previously discussed</a>, each word will have an arbitrary number of leading special characters <code class="language-plaintext highlighter-rouge">"."</code>, and one trailing <code class="language-plaintext highlighter-rouge">"."</code>. In this case, the vocabulary of characters \(\{\cdot, a, b\}\) is of size <code class="language-plaintext highlighter-rouge">L=3</code>.</p> <p>We split \(\widetilde X\) into a train and a test set (we don‚Äôt use validation as we will not tune hyperparameters at this stage, but just assess performance).</p> \[\widetilde X_\text{tr} = \{ab, bb\}, \qquad \widetilde X_\text{te} = \{ba\}.\] <p>In fact, our generative models will make predictions for every character in the words \(y^i\), given the previous ones \(x^i\). Effectively, our dataset consists of nine tuples \({( x^i, y^i)}_{i=1}^N\)</p> \[X_\text{tr} = \{(\cdot,a), (\cdot a,b), (\cdot ab,\cdot), (\cdot,b), (\cdot b, b), (\cdot bb, \cdot)\}, \qquad X_\text{te} = \{(\cdot,b), (\cdot b, a), (\cdot ba, \cdot)\}.\] <p>where the number of leading <code class="language-plaintext highlighter-rouge">"."</code> depends on the model. For example, 1-gram model we need no leading <code class="language-plaintext highlighter-rouge">"."</code> as no context is needed for predicting the leading characters in a word. The model would fit the train set<br> by counting the number of occurrences of each character in the vocabulary, and make a normalized probability out of it:</p> <table> <thead> <tr> <th style="text-align: center">\(P^{1\text{gram}}\)</th> <th style="text-align: center">\(\cdot\)</th> <th style="text-align: center">\(a\)</th> <th style="text-align: center">\(b\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">¬†</td> <td style="text-align: center">\(1/3\)</td> <td style="text-align: center">\(1/6\)</td> <td style="text-align: center">\(1/2\)</td> </tr> </tbody> </table> <p><br><br> For the 2-gram model of \(X_\text{tr}\) we need one leading <code class="language-plaintext highlighter-rouge">"."</code> to give the context for predicting the first character. The model consists of \(L^2=9\) numbers, the normalized counts of each tuple:</p> <table> <thead> <tr> <th style="text-align: center">\(P^{2\text{gram}}\)</th> <th style="text-align: center">\(\cdot\)</th> <th style="text-align: center">\(a\)</th> <th style="text-align: center">\(b\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">\(\cdot\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(1/2\)</td> <td style="text-align: center">\(1/2\)</td> </tr> <tr> <td style="text-align: center">\(a\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(1\)</td> </tr> <tr> <td style="text-align: center">\(b\)</td> <td style="text-align: center">\(2/3\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(1/3\)</td> </tr> </tbody> </table> <p><br><br> From the models, we can compute the train and test losses, \(\mathcal{L}(P^\text{ngram}\vert X_\text{tr/te})\), using Eq.\eqref{eq:nll}. We report the results in the table below, togheter with the 0-gram model corresponding to random guessing.</p> <table> <thead> <tr> <th style="text-align: center">\(\mathcal{L}\)</th> <th style="text-align: center">train</th> <th style="text-align: center">test</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">0-gram</td> <td style="text-align: center">1.10</td> <td style="text-align: center">1.10</td> </tr> <tr> <td style="text-align: center">1-gram</td> <td style="text-align: center">1.01</td> <td style="text-align: center">1.19</td> </tr> <tr> <td style="text-align: center">2-gram</td> <td style="text-align: center">0.55</td> <td style="text-align: center">\(+\infty\)</td> </tr> </tbody> </table> <p><br><br> A few comments on the results:</p> <ol> <li>The <strong>0-gram</strong> model has no information on the training data, hence its performance is the same on both splits. We can keep in mind the number 1.10 as a benchmark.</li> <li>The <strong>1-gram</strong> model has learned something from the data, hence reducing its training loss w.r.t. random guessing. The large test loss, on the other side, is a first manifestation of overfitting.</li> <li>The <strong>2-gram</strong> model has a significantly lower training loss. However, its test loss diverges. This is because there are tuples in the test set \(\{(b, a), (a, \cdot)\}\) to which the model assigns zero probability‚Äîsince they were not present in the training set‚Äîmaking the log diverge.</li> </ol> <p>One simple way to <em>regularize</em> this divergence is to introduce <em>smoothing</em> to our model. Smoothing consists in assigning a finite probability to all tuples, even if they are not present in the training set. This can be done by add one to all tuples countings, before producing the normalized probability. The resulting 2-gram model would be</p> <table> <thead> <tr> <th style="text-align: center">\(P^{2\text{gram}}\)</th> <th style="text-align: center">\(\cdot\)</th> <th style="text-align: center">\(a\)</th> <th style="text-align: center">\(b\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">\(\cdot\)</td> <td style="text-align: center">0.2</td> <td style="text-align: center">0.4</td> <td style="text-align: center">0.4</td> </tr> <tr> <td style="text-align: center">\(a\)</td> <td style="text-align: center">0.25</td> <td style="text-align: center">0.25</td> <td style="text-align: center">0.5</td> </tr> <tr> <td style="text-align: center">\(b\)</td> <td style="text-align: center">0.5</td> <td style="text-align: center">0.17</td> <td style="text-align: center">0.33</td> </tr> </tbody> </table> <p><br><br> And the resulting log-likelihoods read</p> <table> <thead> <tr> <th style="text-align: center">\(\mathcal{L}\)</th> <th style="text-align: center">train</th> <th style="text-align: center">test</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">0-gram</td> <td style="text-align: center">1.10</td> <td style="text-align: center">1.10</td> </tr> <tr> <td style="text-align: center">1-gram</td> <td style="text-align: center">1.02</td> <td style="text-align: center">1.14</td> </tr> <tr> <td style="text-align: center">2-gram</td> <td style="text-align: center">0.84</td> <td style="text-align: center">1.36</td> </tr> </tbody> </table> <p><br><br> We hence eliminated the divergence and observe, as a general trend, that the training loss increased, while the test loss decreased. This is a common feature of <em>regularization methods</em>.</p> <h3 id="back-to-italian-names">Back to Italian names</h3> <p>Now that we have a better sense of the game we are playing, we can go back to our more complicated Italian names dataset and decide which <code class="language-plaintext highlighter-rouge">n</code>-gram model works the best!</p> <p>First, we randomly split our dataset into a training and a test set (90% / 10%).</p> <p>Also, we have learned that some amount of smoothing is necessary, as we go to large <code class="language-plaintext highlighter-rouge">n</code>, to avoid the divergence of the log-likelihood.</p> <p>We can now compute the log-likelihoods for the two splits, for n-gram models of different order and smoothing values (see Figure below). The computations can be found in <a href="https://github.com/leonardopetrini/learning-italian-names/blob/main/likelihood.ipynb" rel="external nofollow noopener" target="_blank">this jupyter notebook</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ngrams_overfitting-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ngrams_overfitting-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ngrams_overfitting-1400.webp"></source> <img src="/assets/img/ngrams_overfitting.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overfitting in n-gram models: loss functions vs. n. </div> <p>Some observations.</p> <ol> <li>Smoothing has little effect at small <code class="language-plaintext highlighter-rouge">n</code>, while it largely mitigates overfitting for large <code class="language-plaintext highlighter-rouge">n</code>: the regularization effect we discussed above.</li> <li>For small smoothing, the optimum is reached for <code class="language-plaintext highlighter-rouge">n=3</code>, <code class="language-plaintext highlighter-rouge">loss = 2.01</code>.</li> <li>Larger smoothing allows for a better optimum at <code class="language-plaintext highlighter-rouge">n=4</code>, <code class="language-plaintext highlighter-rouge">loss = 1.83</code>.</li> </ol> <p><strong><em>Note:</em></strong> We should be careful with the smoothing parameter, as it gives a non-zero probability to n-tuples of characters that never occurr, which can result in very weird generated words.</p> <h2 id="summary">Summary</h2> <p>In this post, we introduced a measure of <em>model goodness</em> that derives from simple notions of probability. By measuring the negative log-likelihood of <code class="language-plaintext highlighter-rouge">n</code>-gram models, we now have a quantitative way to assess their <em>goodness</em>. Interestingly enough, this is in line with the anecdotal observations we made in the previous post about <code class="language-plaintext highlighter-rouge">3</code>-grams being a better model then <code class="language-plaintext highlighter-rouge">6</code>-grams, as it produces fairly reasonable names, without just reproducing training set ones.</p> <p>Now that we reached a good understanding of our problem: how to build simple models that capture data statistics, how to properly measure how good a model is, given the data, we are ready to dive in the realm of <strong><em>artificial neural networks</em></strong> üß†!</p> <p>Stay tuned. üìª</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2023 Leonardo Petrini. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>