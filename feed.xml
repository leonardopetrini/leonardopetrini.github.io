<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="https://leopetrini.me/feed.xml" rel="self" type="application/atom+xml"/><link href="https://leopetrini.me/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-01-05T16:34:38+00:00</updated><id>https://leopetrini.me/feed.xml</id><title type="html">blank</title><subtitle>Leonardo Petrini personal webpage. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Cross Entropy Loss: a log-likelihood in disguise ü•∑</title><link href="https://leopetrini.me/blog/2023/cross-entropy/" rel="alternate" type="text/html" title="The Cross Entropy Loss: a log-likelihood in disguise ü•∑"/><published>2023-01-05T07:00:00+00:00</published><updated>2023-01-05T07:00:00+00:00</updated><id>https://leopetrini.me/blog/2023/cross-entropy</id><content type="html" xml:base="https://leopetrini.me/blog/2023/cross-entropy/"><![CDATA[<p>The <strong>cross-entropy loss</strong> that is so much used in classification tasks is nothing but‚Ä¶ the <strong>negative log-likelihood</strong> in disguise! ü•∏ In this short post, we will clarify this point and make the connection explicit.</p> <p>Let‚Äôs consider a dataset of input-label pairs \(\{x^i, y^i\}_{i=1}^N\) similar to what we considered in the <a href="/blog/2022/likelihood/">last post</a>. For Italian names, \(x^i\) consisted of a set of characters that are used to predict \(y^i\), the next character. The model we used, <code class="language-plaintext highlighter-rouge">n</code>-grams, consists of look-up-tables that, given \(x^i\), return an array of normalized probabilities that the model assigns to the next character \(y^i\) being the first, second,‚Ä¶ <code class="language-plaintext highlighter-rouge">L</code>-th element in the vocabulary. With the use of some notation, our model \(\mathcal{M}\) that takes as input the \(x^i\)‚Äôs and returns a length-<code class="language-plaintext highlighter-rouge">L</code> vector of probabilities \(p^i = \mathcal{M}(x^i)\). Neural networks, as we will see, also fit in this setting.</p> <p>We recall that the log-likelihood of a given sample corresponds to the negative log probability that the model assigns to the correct class \(y^i\):</p> \[\mathcal{l}^i = -\log [\mathcal{M}(x^i)]_{y^i} = -\log [p^i]_{y^i}\] <p>where we used the notation \([\mathbf{x}]_{i}\) to indicate the \(i\)-th element of vector \(\mathbf{x}\) and lowercase \(\mathcal{l}\) to indicate the loss of one single data point.</p> <p>At this stage, the \(y^i\)‚Äôs take values in \(\{0, 1,\dots, L\}\). If we instead represent them in <a href="https://en.wikipedia.org/wiki/One-hot">one-hot-encoding</a>, i.e.</p> \[\begin{aligned} \cdot &amp;\rightarrow 0 \rightarrow [1,0,0,..., 0] \\ a &amp;\rightarrow 1 \rightarrow [0,1,0,..., 0]\\ &amp;\qquad\quad\vdots\\ √π &amp;\rightarrow L \rightarrow [0, ...,0,0,1]\\ \end{aligned}\] <p>the negative log-likelihood of a sample simply reads</p> \[\mathcal{l}^i = -\sum_{c=1}^L y^i_c\log (p^i_c),\] <p>that is <strong>the definition of cross-entropy loss</strong>!!</p> <p>I hope that this helped shed some light üî¶ on the widespread use of this seemingly obscure loss function, and its justification.</p>]]></content><author><name></name></author><category term="ml-projects"/><category term="blogposts"/><category term="machine-learning"/><category term="likelihood"/><summary type="html"><![CDATA[What's the connection between the cross entropy loss and the log-likelihood?]]></summary></entry><entry><title type="html">Italian names generator ü§å (Part II)</title><link href="https://leopetrini.me/blog/2022/likelihood/" rel="alternate" type="text/html" title="Italian names generator ü§å (Part II)"/><published>2022-12-18T07:00:00+00:00</published><updated>2022-12-18T07:00:00+00:00</updated><id>https://leopetrini.me/blog/2022/likelihood</id><content type="html" xml:base="https://leopetrini.me/blog/2022/likelihood/"><![CDATA[<p>Back to our series about <strong>generating Italian names</strong>! <a href="/blog/2022/italian-names/">Last time</a> we built <em>n-gram</em> models for our data, seen that results improve as <code class="language-plaintext highlighter-rouge">n</code> gets larger, but also discussed their limitations. In this post, we are going to define a measure of model <em>goodness</em> to quantitatively assess which <code class="language-plaintext highlighter-rouge">n</code> works best with the data we have.</p> <p>The notebook reproducing the results shown in this post can be found here: <a href="https://github.com/leonardopetrini/learning-italian-names/blob/main/likelihood.ipynb">github/leonardopetrini/learning-italian-names/likelihood.ipynb</a>.</p> <h2 id="introducing-the-likelihood">Introducing the <em>likelihood</em></h2> <p>A very natural measure of how good is our generative model in reproducing the statistical properties of the data is the probability that such data was generated from the model in the first place.</p> <p>Intuitively, if the data have structure, and our model was just randomly guessing, the probability that it generated the data would be very low, while a model correctly reproducing some data statistics (e.g. letter <code class="language-plaintext highlighter-rouge">n</code> often comes after letter <code class="language-plaintext highlighter-rouge">i</code>) would more <em>likely</em> have generated such data.</p> <p>The measure of <em>how likely</em> some data \(\widetilde X = \{\widetilde x^i\}_{i=1}^m\) are, given a model \(\mathcal{M}\), is commonly called <strong>likelihood</strong> and is defined as,</p> \[P(\widetilde X\,\vert\, \mathcal{M}) = \prod_{i=1}^m P(\ \widetilde x^i\,\vert\, \mathcal{M}),\] <p>where data points are assumed to be <em>independent</em> of each other (allowing to take the product) and <em>identically distributed</em> (allowing the same \(P\) for all samples). The larger the likelihood the better the model.</p> <p>For convenience, we usually work with the log of this quantity. Moreover, we divide by the total number of samples \(m\) and add a negative sign. This is because:</p> <ol> <li>The likelihood can take very tiny values as it is the product of \(m\) numbers in \([0, 1]\), and \(m\) can be arbitrarily large. Taking the \(\log\) makes it more tractable. Also, the \(\log\) is monotonic, hence it does not change the argmax of this function with respect to the model‚Äôs parameters.</li> <li>The \(\frac{1}{m}\) makes it an average quantity over samples so that it takes reasonable values (\(\mathcal{O}(1)\)) and it does not depend on the dataset size (in physics, we would call the average log-likelihood and <a href="https://en.wikipedia.org/wiki/Intensive_and_extensive_properties">intensive</a> quantity: a property of a system that does not depend on the system size).</li> <li>The negative sign makes it positive and bounded from below by zero. Also, it makes it interpretable as a <em>cost function</em>, a very common object in machine learning (ML).</li> </ol> <p>To summarize, we define the average <strong>negative log-likelihood</strong> as</p> <p>\begin{equation} \mathcal{L}(\mathcal{M}\,\vert\, \widetilde X) = -\frac{1}{m}\sum_{i=1}^m \log P(\ \widetilde x^i\,\vert\, \mathcal{M}) \label{eq:nll} \end{equation}</p> <p>where the order of \(\widetilde X\) and \(\mathcal{M}\) is reversed in the log-likelihood as it is commonly seen as a function of the model, given the data.</p> <p>This quantity is the cost function we aim at minimizing, and will support our choice of a model/set of parameters over another. In the following section, we‚Äôll see how to make proper use of it!</p> <h2 id="train-validate-test">Train, validate, test!</h2> <p><strong>‚ùì What makes a <em>good</em> machine learning model?</strong></p> <p>In the previous post, we have discussed two properties we would like our generative model to have:</p> <ul> <li>Ability to generate <strong>reasonable examples</strong> that look like the training ones;</li> <li>Ability to generate <strong>new examples</strong>, i.e. different than the one used for training.</li> </ul> <p>On the contrary, a model is bad if it generates examples that are reasonable but just copies of training samples. Or if it generates <strong>new</strong> examples that have nothing to do with the original ones (i.e. noise).</p> <p>In ML jargon, a <strong>good</strong> machine learning model has <strong>good</strong> generalization capabilities. This means that model <code class="language-plaintext highlighter-rouge">A</code> is better than model <code class="language-plaintext highlighter-rouge">B</code> if, after training, model <code class="language-plaintext highlighter-rouge">A</code> has a lower log-likelihood on <strong>new samples</strong> that did not belong to the training data, but that ideally come from the <strong>same probability distrubution</strong>.</p> <p>For this reason, we must evaluate the log-likelihood of our models on a different dataset than the one we have used for training.</p> <p>In particular, a common pipeline is to randomly split the dataset at our disposition into three parts (say 80%, 10%, 10%), the <strong>training</strong>, <strong>validation</strong> and <strong>test</strong> sets. The first is the one used for training the model, the second is used to have a sense of the model generalization capabilities, and accordingly tune the model <em>hyperparameters</em> (e.g. the value of <code class="language-plaintext highlighter-rouge">n</code> in <code class="language-plaintext highlighter-rouge">n</code>-grams). Finally, the third must be only sparingly used, usually at the very end of our ML pipeline, to get an unbiased estimate of the model performance.</p> <p>Splitting the dataset into three parts makes our ML pipeline more robust to <em>overfitting</em>, both with respect to the models‚Äô parameters and to the manually-tuned model hyperparameters. We‚Äôll see an example of overfitting below.</p> <h2 id="likelihood-and-word-models">Likelihood and word models</h2> <p>Now we have all the tools to start playing with our beloved Italian names!</p> <h3 id="a-toy-example">A toy example</h3> <p>As a warm-up, we introduce a <em>toy</em> words dataset with just three words:</p> \[\widetilde X = \{ab, bb, ba\}.\] <p>As we <a href="/blog/2022/italian-names/">previously discussed</a>, each word will have an arbitrary number of leading special characters <code class="language-plaintext highlighter-rouge">"."</code>, and one trailing <code class="language-plaintext highlighter-rouge">"."</code>. In this case, the vocabulary of characters \(\{\cdot, a, b\}\) is of size <code class="language-plaintext highlighter-rouge">L=3</code>.</p> <p>We split \(\widetilde X\) into a train and a test set (we don‚Äôt use validation as we will not tune hyperparameters at this stage, but just assess performance).</p> \[\widetilde X_\text{tr} = \{ab, bb\}, \qquad \widetilde X_\text{te} = \{ba\}.\] <p>In fact, our generative models will make predictions for every character in the words \(y^i\), given the previous ones \(x^i\). Effectively, our dataset consists of nine tuples \({( x^i, y^i)}_{i=1}^N\)</p> \[X_\text{tr} = \{(\cdot,a), (\cdot a,b), (\cdot ab,\cdot), (\cdot,b), (\cdot b, b), (\cdot bb, \cdot)\}, \qquad X_\text{te} = \{(\cdot,b), (\cdot b, a), (\cdot ba, \cdot)\}.\] <p>where the number of leading <code class="language-plaintext highlighter-rouge">"."</code> depends on the model. For example, 1-gram model we need no leading <code class="language-plaintext highlighter-rouge">"."</code> as no context is needed for predicting the leading characters in a word. The model would fit the train set<br/> by counting the number of occurrences of each character in the vocabulary, and make a normalized probability out of it:</p> <table> <thead> <tr> <th style="text-align: center">\(P^{1\text{gram}}\)</th> <th style="text-align: center">\(\cdot\)</th> <th style="text-align: center">\(a\)</th> <th style="text-align: center">\(b\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">¬†</td> <td style="text-align: center">\(1/3\)</td> <td style="text-align: center">\(1/6\)</td> <td style="text-align: center">\(1/2\)</td> </tr> </tbody> </table> <p><br/><br/> For the 2-gram model of \(X_\text{tr}\) we need one leading <code class="language-plaintext highlighter-rouge">"."</code> to give the context for predicting the first character. The model consists of \(L^2=9\) numbers, the normalized counts of each tuple:</p> <table> <thead> <tr> <th style="text-align: center">\(P^{2\text{gram}}\)</th> <th style="text-align: center">\(\cdot\)</th> <th style="text-align: center">\(a\)</th> <th style="text-align: center">\(b\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">\(\cdot\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(1/2\)</td> <td style="text-align: center">\(1/2\)</td> </tr> <tr> <td style="text-align: center">\(a\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(1\)</td> </tr> <tr> <td style="text-align: center">\(b\)</td> <td style="text-align: center">\(2/3\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(1/3\)</td> </tr> </tbody> </table> <p><br/><br/> From the models, we can compute the train and test losses, \(\mathcal{L}(P^\text{ngram}\vert X_\text{tr/te})\), using Eq.\eqref{eq:nll}. We report the results in the table below, togheter with the 0-gram model corresponding to random guessing.</p> <table> <thead> <tr> <th style="text-align: center">\(\mathcal{L}\)</th> <th style="text-align: center">train</th> <th style="text-align: center">test</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">0-gram</td> <td style="text-align: center">1.10</td> <td style="text-align: center">1.10</td> </tr> <tr> <td style="text-align: center">1-gram</td> <td style="text-align: center">1.01</td> <td style="text-align: center">1.19</td> </tr> <tr> <td style="text-align: center">2-gram</td> <td style="text-align: center">0.55</td> <td style="text-align: center">\(+\infty\)</td> </tr> </tbody> </table> <p><br/><br/> A few comments on the results:</p> <ol> <li>The <strong>0-gram</strong> model has no information on the training data, hence its performance is the same on both splits. We can keep in mind the number 1.10 as a benchmark.</li> <li>The <strong>1-gram</strong> model has learned something from the data, hence reducing its training loss w.r.t. random guessing. The large test loss, on the other side, is a first manifestation of overfitting.</li> <li>The <strong>2-gram</strong> model has a significantly lower training loss. However, its test loss diverges. This is because there are tuples in the test set \(\{(b, a), (a, \cdot)\}\) to which the model assigns zero probability‚Äîsince they were not present in the training set‚Äîmaking the log diverge.</li> </ol> <p>One simple way to <em>regularize</em> this divergence is to introduce <em>smoothing</em> to our model. Smoothing consists in assigning a finite probability to all tuples, even if they are not present in the training set. This can be done by add one to all tuples countings, before producing the normalized probability. The resulting 2-gram model would be</p> <table> <thead> <tr> <th style="text-align: center">\(P^{2\text{gram}}\)</th> <th style="text-align: center">\(\cdot\)</th> <th style="text-align: center">\(a\)</th> <th style="text-align: center">\(b\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">\(\cdot\)</td> <td style="text-align: center">0.2</td> <td style="text-align: center">0.4</td> <td style="text-align: center">0.4</td> </tr> <tr> <td style="text-align: center">\(a\)</td> <td style="text-align: center">0.25</td> <td style="text-align: center">0.25</td> <td style="text-align: center">0.5</td> </tr> <tr> <td style="text-align: center">\(b\)</td> <td style="text-align: center">0.5</td> <td style="text-align: center">0.17</td> <td style="text-align: center">0.33</td> </tr> </tbody> </table> <p><br/><br/> And the resulting log-likelihoods read</p> <table> <thead> <tr> <th style="text-align: center">\(\mathcal{L}\)</th> <th style="text-align: center">train</th> <th style="text-align: center">test</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">0-gram</td> <td style="text-align: center">1.10</td> <td style="text-align: center">1.10</td> </tr> <tr> <td style="text-align: center">1-gram</td> <td style="text-align: center">1.02</td> <td style="text-align: center">1.14</td> </tr> <tr> <td style="text-align: center">2-gram</td> <td style="text-align: center">0.84</td> <td style="text-align: center">1.36</td> </tr> </tbody> </table> <p><br/><br/> We hence eliminated the divergence and observe, as a general trend, that the training loss increased, while the test loss decreased. This is a common feature of <em>regularization methods</em>.</p> <h3 id="back-to-italian-names">Back to Italian names</h3> <p>Now that we have a better sense of the game we are playing, we can go back to our more complicated Italian names dataset and decide which <code class="language-plaintext highlighter-rouge">n</code>-gram model works the best!</p> <p>First, we randomly split our dataset into a training and a test set (90% / 10%).</p> <p>Also, we have learned that some amount of smoothing is necessary, as we go to large <code class="language-plaintext highlighter-rouge">n</code>, to avoid the divergence of the log-likelihood.</p> <p>We can now compute the log-likelihoods for the two splits, for n-gram models of different order and smoothing values (see Figure below). The computations can be found in <a href="https://github.com/leonardopetrini/learning-italian-names/blob/main/likelihood.ipynb">this jupyter notebook</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ngrams_overfitting-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ngrams_overfitting-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ngrams_overfitting-1400.webp"/> <img src="/assets/img/ngrams_overfitting.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Overfitting in n-gram models: loss functions vs. n. </div> <p>Some observations.</p> <ol> <li>Smoothing has little effect at small <code class="language-plaintext highlighter-rouge">n</code>, while it largely mitigates overfitting for large <code class="language-plaintext highlighter-rouge">n</code>: the regularization effect we discussed above.</li> <li>For small smoothing, the optimum is reached for <code class="language-plaintext highlighter-rouge">n=3</code>, <code class="language-plaintext highlighter-rouge">loss = 2.01</code>.</li> <li>Larger smoothing allows for a better optimum at <code class="language-plaintext highlighter-rouge">n=4</code>, <code class="language-plaintext highlighter-rouge">loss = 1.83</code>.</li> </ol> <p><strong><em>Note:</em></strong> We should be careful with the smoothing parameter, as it gives a non-zero probability to n-tuples of characters that never occurr, which can result in very weird generated words.</p> <h2 id="summary">Summary</h2> <p>In this post, we introduced a measure of <em>model goodness</em> that derives from simple notions of probability. By measuring the negative log-likelihood of <code class="language-plaintext highlighter-rouge">n</code>-gram models, we now have a quantitative way to assess their <em>goodness</em>. Interestingly enough, this is in line with the anecdotal observations we made in the previous post about <code class="language-plaintext highlighter-rouge">3</code>-grams being a better model then <code class="language-plaintext highlighter-rouge">6</code>-grams, as it produces fairly reasonable names, without just reproducing training set ones.</p> <p>Now that we reached a good understanding of our problem: how to build simple models that capture data statistics, how to properly measure how good a model is, given the data, we are ready to dive in the realm of <strong><em>artificial neural networks</em></strong> üß†!</p> <p>Stay tuned. üìª</p>]]></content><author><name></name></author><category term="ml-projects"/><category term="blogposts"/><category term="nlp"/><category term="machine-learning"/><category term="likelihood"/><summary type="html"><![CDATA[measuring the *goodness* of generative models: the likelihood]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://leopetrini.me/likelihood-def.png"/><media:content medium="image" url="https://leopetrini.me/likelihood-def.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">my bread recipe üçû</title><link href="https://leopetrini.me/blog/2022/bread/" rel="alternate" type="text/html" title="my bread recipe üçû"/><published>2022-12-11T08:00:00+00:00</published><updated>2022-12-11T08:00:00+00:00</updated><id>https://leopetrini.me/blog/2022/bread</id><content type="html" xml:base="https://leopetrini.me/blog/2022/bread/"><![CDATA[<p>Sometimes I make bread at home. Each new loaf is a step toward the best values for the parameters <code class="language-plaintext highlighter-rouge">{flour, water, salt, sugar, yeast, temperature}</code> that maximize the reward I get by eating it. Then, I stop doing bread for weeks and I just forget the recipe.</p> <p>This post serves as the place where to find my bread recipe when I will need it.</p> <p><em>Disclaimer:</em> This recipe makes use of a bread machine, courtesy of my former flatmate.</p> <h2 id="ingredients-for-half-a-kilo-of-bread">Ingredients for half a kilo of bread</h2> <table> <thead> <tr> <th>Ingredient</th> <th>Quantity</th> </tr> </thead> <tbody> <tr> <td>üåæ flour <em>Tipo 1</em>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†</td> <td>350g</td> </tr> <tr> <td>üíß Water (~30¬∞C)</td> <td>210g</td> </tr> <tr> <td>üç¨ Sugar</td> <td>12g</td> </tr> <tr> <td>ü´í Oil</td> <td>10g</td> </tr> <tr> <td>üßÇ Salt</td> <td>6g</td> </tr> <tr> <td>‚è´ Fresh yeast</td> <td>6g</td> </tr> </tbody> </table> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bread/1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bread/1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bread/1-1400.webp"/> <img src="/assets/img/bread/1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bread/0-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bread/0-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bread/0-1400.webp"/> <img src="/assets/img/bread/0.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="flour-which-kind">Flour: which kind?</h2> <p>Unfortunately, each country has its particular way of classifying flour types, from the more scientific to the more qualitative ones. Below is the best conversion table I could put together by scraping the web, hope this can be useful to you fellow expat!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bread/table-flour-types-1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bread/table-flour-types-1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bread/table-flour-types-1-1400.webp"/> <img src="/assets/img/bread/table-flour-types-1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bread/table-flour-types-2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bread/table-flour-types-2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bread/table-flour-types-2-1400.webp"/> <img src="/assets/img/bread/table-flour-types-2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For making bread, I use <em>farina di tipo 1</em>, #3 in the conversion table.</p> <h2 id="yeast-and-water-temperature-">Yeast and water temperature üö∞</h2> <p>Yeast feeds on sugar and produces \(CO_2\) bubbles which make the loaf inflate. This process is called fermentation, analogous to the one used to make alcoholic beverages (ethanol is produced here as well).</p> <p>We would like the yeast to produce enough carbon dioxide so that our bread builds a nice bubbly structure, which is then solidified by heat. However, too much \(CO_2\) before the bread is baked and the dough will <a href="https://en.wikipedia.org/wiki/Yield_(engineering)">yield</a>, make the \(CO_2\) escape and deflate/collapse.</p> <p>We have three ways of controlling the fermentation process:</p> <ol> <li>Temperature (essentially water temperature);</li> <li>Amount of sugar;</li> <li>Amount of salt.</li> </ol> <p>Increasing temperature (up to ~35¬∞C<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>) accelerates the fermentation, similar to adding small amounts of sugar for the yeast to feed onto. Adding salt or too much sugar, however, will increase the osmotic pressure and negatively impact yeast viability.</p> <p>Now that we understand a bit more of the process behind rising bread, we understand that there are probably many optima in the space of these three parameters that allow for a loaf of good bread.</p> <p>For the quantity of sugar and salt indicated above, I find that to obtain a nice bubbly loaf, the water temperature needs to be higher than room temperature, but not too hot. This is around 30¬∞C or 86¬∞F, warm at the touch. üå°Ô∏è</p> <h2 id="method">Method</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bread/3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bread/3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bread/3-1400.webp"/> <img src="/assets/img/bread/3.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bread/4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bread/4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bread/4-1400.webp"/> <img src="/assets/img/bread/4.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bread/6-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bread/6-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bread/6-1400.webp"/> <img src="/assets/img/bread/6.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bread/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bread/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bread/7-1400.webp"/> <img src="/assets/img/bread/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Pictures of steps 2, 4, 6, and 8. </div> <ol> <li>Add some of the total amounts of water in a cup together with the yeast and mix until it melts.</li> <li>Add everything else to the bread machine container, paying attention to keeping the salt on the opposite side w.r.t. the sugar.</li> <li>Add the melted yeast to the sugar side.</li> <li>Start the machine. For 500g of bread, I use a 2h55m program, medium crust.</li> <li>Pay attention for a few minutes that all the flour is being incorporated into the dough. Otherwise, help it with a spoon. The container should be clean and all the ingredients should form a unique solid dough.</li> <li>After five to ten minutes, the dough should be hydrated enough that sticks to the container, yet keeps a round shape. <ul> <li>If it is not round, add some flour;</li> <li>If it‚Äôs not sticky, add some water.</li> </ul> </li> <li>Be Patient üïêüïùüïì, your job is almost done!</li> <li>When the program is finished, get the bread out üçû.</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bread/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bread/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bread/9-1400.webp"/> <img src="/assets/img/bread/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol start="9"> <li>Remove the <em>metal thing</em> from the bread.</li> <li>Make a nicely stuffed <em>panino</em>! üòã</li> </ol> <p><strong><em>Note:</em></strong> even if the bread loses its freshness in a couple of days, it‚Äôs still great for toasted sandwiches ü•™ for a week or so!</p> <p>I leave you with some <em>panini</em> pictures from the tops of the Swiss Alps üá®üá≠‚õ∞Ô∏è!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/panini/panino1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/panini/panino1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/panini/panino1-1400.webp"/> <img src="/assets/img/panini/panino1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/panini/panino2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/panini/panino2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/panini/panino2-1400.webp"/> <img src="/assets/img/panini/panino2.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/panini/panino3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/panini/panino3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/panini/panino3-1400.webp"/> <img src="/assets/img/panini/panino3.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/panini/panino4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/panini/panino4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/panini/panino4-1400.webp"/> <img src="/assets/img/panini/panino4.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/panini/panino5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/panini/panino5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/panini/panino5-1400.webp"/> <img src="/assets/img/panini/panino5.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Struyf, N., Van der Maelen, E., Hemdane, S., Verspreet, J., Verstrepen, K.J. and Courtin, C.M. (2017), Bread Dough and Baker‚Äôs Yeast: An Uplifting Synergy. Comprehensive Reviews in Food Science and Food Safety, 16: 850-867. https://doi.org/10.1111/1541-4337.12282¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="recipes"/><category term="recipe"/><category term="food"/><summary type="html"><![CDATA[every time I find a bread recipe I like, I lose it]]></summary></entry><entry><title type="html">Italian names generator ü§å (Part I)</title><link href="https://leopetrini.me/blog/2022/italian-names/" rel="alternate" type="text/html" title="Italian names generator ü§å (Part I)"/><published>2022-12-04T15:42:00+00:00</published><updated>2022-12-04T15:42:00+00:00</updated><id>https://leopetrini.me/blog/2022/italian-names</id><content type="html" xml:base="https://leopetrini.me/blog/2022/italian-names/"><![CDATA[<p>This series of posts is largely inspired by Andrej Karpathy‚Äôs <a href="https://github.com/karpathy/makemore"><em>makemore</em></a>. In my experience as an ML researcher, I‚Äôve never found a more clear and sharp teacher than Andrej, his ability to walk you through the basics without being pedantic, and give insightful comments on the way, I find it unique. I highly recommend his (ongoing) YouTube series on <a href="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: from Zero to Hero</a>.</p> <p>So, let‚Äôs start! As the title suggests, we are going to build generative models for <em>Italian first names</em> that will learn from examples. Such models will be able to generate new words from scratch or complete a series of characters. I will make use of two names datasets publicly available on GitHub:</p> <ol> <li><a href="https://gist.github.com/pdesterlich/2562329"><code class="language-plaintext highlighter-rouge">names_1</code></a> contains ~9k first names of people living in Italy, not necessarily <em>strictly</em> Italian. There are e.g. some French names;</li> <li><a href="https://github.com/filippotoso/nomi-cognomi-italiani/blob/master/json/nomi-italiani.json"><code class="language-plaintext highlighter-rouge">names_2</code></a> contains ~1.7k first names.</li> </ol> <p>Some examples from <code class="language-plaintext highlighter-rouge">names_1</code>,</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ranuccio palmerio eustacchia quentalina gesuina azaea finaldo oriana 
</code></pre></div></div> <p>and <code class="language-plaintext highlighter-rouge">names_2</code>,</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>romoaldo donatella nicoletta aristeo natalia rainelda serafina susanna
</code></pre></div></div> <p>In the following, I will consider the larger dataset, <code class="language-plaintext highlighter-rouge">names_1</code>.</p> <p>The datasets, together with the notebook reproducing the results shown in this post can be found at <a href="https://github.com/leonardopetrini/learning-italian-names">github/leonardopetrini/learning-italian-names</a>.</p> <h2 id="0-th-order-model-random-guessing">0-th order model: random guessing</h2> <p>Let‚Äôs start with the simplest thing we could do, and a very bad baseline: <em>random guessing</em>. I call it a 0th-order method because we give the model no information about the statistics of characters in the words. This point will become clearer later on when we go to higher order.</p> <p>In this context, random guessing consists in taking a dictionary of all the characters that occur in our words dataset and sampling uniformly at random from them.</p> <p>The dictionary of chars appearing in <code class="language-plaintext highlighter-rouge">names_1</code> is the following</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>' ' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'
'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' '√†' '√®' '√¨' '√≤' '√π' 
</code></pre></div></div> <p>You can notice that to the 26 letters of the Latin alphabet we added accented vowels, and the blank, as some Italian names are composite, for a total of <code class="language-plaintext highlighter-rouge">32</code> chars.</p> <p>We add the char <code class="language-plaintext highlighter-rouge">'.'</code> to the dictionary as a placeholder indicating the word end, and append it to all words as well. This makes our dictionary of length <code class="language-plaintext highlighter-rouge">L=33</code>.</p> <p>For generating a word, we will sample any one character uniformly at random (i.e. with probability \(1/L \approx 0.03\)), until we sample the special char <code class="language-plaintext highlighter-rouge">'.'</code>, at which we stop. Samples from such a trivial model will look like this</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code> hxdwsemlxermj√πabp√≤ctq gmamqqvz .
bc√†enn√¨qklrjktrhaulwtkwjjjmcj.
agt.
√πzfcds√π√≤dso.
.
ybt√®√≤.
im√¨uopsgjzwmfekdq√†yh√®fl nlm√¨xsapcqyoo√≤wwy√≤sfvnncfedcwmvvwthfemgsvip√†oh√® vunxdd√†kfugmil√πixv√πccw√¨btrtnd√≤c√πk.
zsyszq√¨.
ktmxktnsypd fxpks pagzsra√†uy√¨yt√®jujj√†iqwjpaqw√®.
 ptazvuwgisdpmfkzmjvyrg√≤lsnaw√≤add√¨tupnkysnakyxh√≤jh√π.
</code></pre></div></div> <p>Clearly not Italian, nor words! ‚ùå</p> <p>Indeed, one of the first things we notice is that words are incredibly longer than typical names. This is because the <code class="language-plaintext highlighter-rouge">'.'</code> will occur, on average, every 33 characters, making the average word length =33, much larger than the average word length in <code class="language-plaintext highlighter-rouge">names_1</code> of <code class="language-plaintext highlighter-rouge">7.1</code>.</p> <p>We start to understand that the first necessary step is to account for the relative occurrence frequency of different characters.</p> <h2 id="1-st-order-model-average-char-occurrence">1-st order model: average char occurrence</h2> <p>In this section, we start to learn from data. As hinted above, the 1st order statistics we can get from our dataset consist of accounting for how often each element in the vocabulary appears in Italian names.</p> <p>To do so, we count how many times (<code class="language-plaintext highlighter-rouge">N</code>) each character appears. Then, we normalize the counting and obtain the probability <code class="language-plaintext highlighter-rouge">p</code> for char occurrence. These values are reported in the table below.</p> <table> <thead> <tr> <th>¬†</th> <th>space</th> <th>a</th> <th>b</th> <th>c</th> <th>d</th> <th>e</th> <th>f</th> <th>g</th> <th>h</th> <th>i</th> <th>j</th> <th>k</th> <th>l</th> <th>m</th> <th>n</th> <th>o</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">N</code></td> <td>102</td> <td>9116</td> <td>792</td> <td>1777</td> <td>2628</td> <td>5653</td> <td>1345</td> <td>1361</td> <td>209</td> <td>8380</td> <td>50</td> <td>38</td> <td>4720</td> <td>1979</td> <td>5487</td> <td>7585</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">p</code></td> <td>0.0014</td> <td>0.1237</td> <td>0.0107</td> <td>0.0241</td> <td>0.0357</td> <td>0.0767</td> <td>0.0183</td> <td>0.0185</td> <td>0.0028</td> <td>0.1137</td> <td>0.0007</td> <td>0.0005</td> <td>0.0641</td> <td>0.0269</td> <td>0.0745</td> <td>0.1029</td> </tr> </tbody> </table> <table> <thead> <tr> <th>¬†</th> <th>p</th> <th>q</th> <th>r</th> <th>s</th> <th>t</th> <th>u</th> <th>v</th> <th>w</th> <th>x</th> <th>y</th> <th>z</th> <th>√†</th> <th>√®</th> <th>√¨</th> <th>√≤</th> <th>√π</th> <th>.</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">N</code></td> <td>709</td> <td>73</td> <td>4884</td> <td>2147</td> <td>2558</td> <td>1141</td> <td>997</td> <td>65</td> <td>9</td> <td>26</td> <td>721</td> <td>4</td> <td>8</td> <td>3</td> <td>3</td> <td>4</td> <td>9111</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">p</code></td> <td>0.0096</td> <td>0.0010</td> <td>0.0663</td> <td>0.0291</td> <td>0.0347</td> <td>0.0155</td> <td>0.0135</td> <td>0.0009</td> <td>0.0001</td> <td>0.0004</td> <td>0.0098</td> <td>0.0001</td> <td>0.0001</td> <td>0.0000</td> <td>0.0000</td> <td>0.0001</td> <td>0.1236</td> </tr> </tbody> </table> <p><br/><br/> Notice that the special character <code class="language-plaintext highlighter-rouge">"."</code> appears 9111 times, which corresponds to the dataset size. Moreover, letters like <code class="language-plaintext highlighter-rouge">j, k, w, x, y</code> are very rare, while vowels are very common, except for the letter <code class="language-plaintext highlighter-rouge">u</code>.</p> <p>Similarly to what we did for random guessing, we sample new words by sequentially sampling characters, this time accordingly to <code class="language-plaintext highlighter-rouge">p</code>, until we hit <code class="language-plaintext highlighter-rouge">"."</code>. Here some results,</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>io.
ttlfo.
iarf.
iefniaaieanie.
uiaahl.
lvpcoroniaadi.
rv o.
inrealo.
aoieueaneaa.
b.
</code></pre></div></div> <p>Words length is now reasonable, and vowels appear much more often, as they should. Still, none of these words can be mistaken for an Italian name.</p> <h2 id="2-nd-order-model-pairwise-correlations">2-nd order model: pairwise correlations</h2> <p>Now we are starting to understand the game of incorporating data statistics into our model. In this step, we include 2nd order statistics, namely <em>pairwise correlations</em> between characters. This is to capture the fact that, e.g. the letter <code class="language-plaintext highlighter-rouge">n</code> often follows the letter <code class="language-plaintext highlighter-rouge">i</code>, <code class="language-plaintext highlighter-rouge">o</code> often follows <code class="language-plaintext highlighter-rouge">n</code> and so on‚Ä¶</p> <p>Similarly to the above case, this is done by making a table of all possible character pairs, computing their occurrence <code class="language-plaintext highlighter-rouge">N</code>, and normalizing it to obtain a probability <code class="language-plaintext highlighter-rouge">p</code>. Then, we look at the current character, we go to the corresponding row of <code class="language-plaintext highlighter-rouge">p</code> and sample the next char accordingly.</p> <p>Now you may ask: how do we choose the first character? To do that, we resort to a simple trick: we append to all words‚Äô beginnings our special <code class="language-plaintext highlighter-rouge">"."</code>, similarly to what we did for word ends. In this way, the row of <code class="language-plaintext highlighter-rouge">p</code> corresponding to <code class="language-plaintext highlighter-rouge">"."</code> will tell us what‚Äôs the probability that a given name starts with any of the letters in the vocabulary.</p> <p>The table <code class="language-plaintext highlighter-rouge">N</code> for char pairs is shown in the figure.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pairwise_n_it_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pairwise_n_it_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pairwise_n_it_1-1400.webp"/> <img src="/assets/img/pairwise_n_it_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Pairs occurrence count $$N(c_i, c_{i+1})$$ </div> <p>Some features of Italian names start to emerge by looking at this table. In the 1st column, for example, we see that most names end with <code class="language-plaintext highlighter-rouge">a</code> or <code class="language-plaintext highlighter-rouge">o</code>, where the first is usually for feminine names, while the second is for masculine ones.</p> <p>Taking a few samples from our model we obtain</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vintona.
mada.
memefio.
o.
jalino.
ricaeto.
pelandinrazona.
wilio.
mia.
zia.
</code></pre></div></div> <p>Start to sound reasonable, don‚Äôt they?!</p> <h2 id="higher-order-models-n-grams">Higher order models: <em>n-grams</em></h2> <p>In the literature, the models we are introducing here are called <a href="https://en.wikipedia.org/wiki/N-gram"><em>n-grams</em></a>, where <code class="language-plaintext highlighter-rouge">n</code> stands for the model <em>order</em>.</p> <p><strong><em>Note:</em></strong> How to sample from higher order models starts to be less intuitive, check the <em>post scriptum</em> below if you are interested in a more detailed walkthrough!</p> <p>Better and better results can be obtained by going to higher and higher order correlations, for example, for <code class="language-plaintext highlighter-rouge">n=3</code> we get,</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vilommanna.
cennelforena.
dino.
do.
cantino.
edo.
lipetto.
curgio.
medazia.
rano.
</code></pre></div></div> <p>‚ùì Could a non-native Italian speaker distinguish these from actual Italian names?</p> <p>It looks like by just replicating 3-rd order statistics from data, our model does a pretty good job! üëä Yet, <em>n-grams</em> have some limitations:</p> <ol> <li><strong>Space.</strong> We need to store a matrix of size <code class="language-plaintext highlighter-rouge">L^n</code>, with <code class="language-plaintext highlighter-rouge">L</code> as the vocabulary size. On my laptop, I can get at most to <code class="language-plaintext highlighter-rouge">n=6</code> before getting to a memory error. For larger vocabulary sizes, this limit is reached even earlier (e.g. for English sentences, <code class="language-plaintext highlighter-rouge">L~10^4</code>).</li> </ol> <p>Here the <code class="language-plaintext highlighter-rouge">n=6</code> results:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>violanda.    1           
emerigo.     1           
cordelia.    1           
mirco.       1           
carolino.    1           
arina.       1           
azzurro.     1           
fiolomea.    0           
martano.     1           
flamiana.    0   
</code></pre></div></div> <ol start="2"> <li> <p><strong>Overfitting.</strong> In these results, I appended a <code class="language-plaintext highlighter-rouge">1</code> when the generated sample belongs to the original dataset. We see that the model learns by heart most of the examples and it can produce little variability beyond that. This is a manifestation of overfitting.</p> </li> <li> <p><strong>Long-range context.</strong> When the average word length gets large (<code class="language-plaintext highlighter-rouge">&gt;&gt;n</code>), n-grams fail to capture long-range correlations between characters.</p> </li> </ol> <p>Do modern artificial neural networks ü§ñ overcome these limitations and give better results? Moreover, how do we decide if a model is better than another, beyond anecdotal evidence?</p> <p>We will give answers to these questions in the following posts on this topic, stay tuned. üìª</p> <p><br/><br/></p> <h2 id="ps-how-to-generate-new-words-from-the-model"><strong>PS:</strong> How to generate new words from the model</h2> <p>We briefly illustrate here how to generate new words given the model. We consider the case <code class="language-plaintext highlighter-rouge">n=3</code>. We built the matrix \(N(c_i, c_{i-1}, c_{i-2})\) by counting the occurrences of the three consecutive chars \(c_{i-2}c_{i-1}c_i\) in the data samples. Notice that all words in the dataset are now augmented with <code class="language-plaintext highlighter-rouge">n-1</code> leading <code class="language-plaintext highlighter-rouge">"."</code> and <code class="language-plaintext highlighter-rouge">1</code> trailing <code class="language-plaintext highlighter-rouge">"."</code>. This is to capture the statistics of the first <code class="language-plaintext highlighter-rouge">n-1</code> characters, which do not have <code class="language-plaintext highlighter-rouge">n-1</code> predecessors.</p> <p>We normalize the first dimension of \(N\) to obtain the conditional probability of a character given the previous <code class="language-plaintext highlighter-rouge">n-1</code>s,</p> \[P(c_i \,\vert\, c_{i-1}, c_{i-2}) = \frac{N(c_i, c_{i-1}, c_{i-2})}{\sum_{c_i} N(c_i, c_{i-1}, c_{i-2})}.\] <p>Finally, to sample a new word, we make use of our special character <code class="language-plaintext highlighter-rouge">"."</code>. The first char is sampled according to \(c_1 \sim P(c \,\vert\, .\,,\, .\,).\) Then, we sample \(c_2\) from \(P(c \,\vert\, c_1,\, .\,)\), \(c_3\) from \(P(c \,\vert\, c_2, c_1)\) and so on, until we sample the stopping char <code class="language-plaintext highlighter-rouge">"."</code>. We got our new word \(c_1c_2\dots c_m\), where \(c_m = \,.\,\).</p>]]></content><author><name></name></author><category term="ml-projects"/><category term="blogposts"/><category term="nlp"/><category term="machine-learning"/><category term="n-grams"/><summary type="html"><![CDATA[building an ML model for generating Italian names]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://leopetrini.me/pairwise_n_it_1.png"/><media:content medium="image" url="https://leopetrini.me/pairwise_n_it_1.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">I start posting!</title><link href="https://leopetrini.me/blog/2022/start/" rel="alternate" type="text/html" title="I start posting!"/><published>2022-11-28T16:40:16+00:00</published><updated>2022-11-28T16:40:16+00:00</updated><id>https://leopetrini.me/blog/2022/start</id><content type="html" xml:base="https://leopetrini.me/blog/2022/start/"><![CDATA[<p>I start to make something with this space! üöÄ</p> <p>I plan to write down posts, probably something in between actual blogposts and <em>tidy</em> personal notes. The <em>why</em> of it is beautifully exemplified in More To That‚Äôs <a href="https://moretothat.com/release-ratio/">The Release Ratio</a>. <em>Bref</em>, I consume a lot of online content and sparingly elaborate on it. When I do so, it is by taking messy notes or sending Telegram messages to myself. This space starts as a way to force me to write tidy notes about things I‚Äôm interested in, and eventually write blogposts if I think there is something interesting I can produce.</p> <p>For those of you that write code, this effort is analogous to forcing yourself to push any piece of code you write to GitHub: nobody will eventually read it, but knowing that everybody possibly <strong><em>could</em></strong> is a strong enough incentive to produce something my future self could rely on, and that is all in one place.</p> <p>Let‚Äôs see how that works out.</p> <p>I leave you with two pictures of the first snow of the season ‚ùÑÔ∏è.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/IMG000-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/IMG000-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/IMG000-1400.webp"/> <img src="/assets/img/IMG000.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/IMG001-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/IMG001-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/IMG001-1400.webp"/> <img src="/assets/img/IMG001.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> November 26, 2022, Swiss Alps üá®üá≠. </div>]]></content><author><name></name></author><category term="notes"/><category term="start"/><summary type="html"><![CDATA[starting a blog, or maybe just a set publicly-available personal notes]]></summary></entry></feed>